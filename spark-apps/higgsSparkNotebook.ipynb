{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "import numpy\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:brown\"> Spark configuration</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"GP over Spark\")\n",
    "if not('sc' in globals()):\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n",
      "2.7\n",
      "local[*]\n",
      "None\n",
      "hadoop\n",
      "GP over Spark\n",
      "local-1573940720471\n",
      "8\n",
      "2\n",
      "2.7.13 (default, Sep 26 2018, 18:42:22) \n",
      "[GCC 6.3.0 20170516]\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)\n",
    "print(sc.pythonVer)\n",
    "print(sc.master)\n",
    "print(str(sc.sparkHome))\n",
    "print(str(sc.sparkUser()))\n",
    "print(sc.appName)\n",
    "print(sc.applicationId)\n",
    "print(sc.defaultParallelism)\n",
    "print(sc.defaultMinPartitions)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingRDD = sc.textFile('hdfs://nodemaster:9000/user/hadoop/HIGGS_Training_Scaled_10500.csv').map(lambda line : [float(x) for x in line.split(',')]).cache()\n",
    "TestRDD = sc.textFile('hdfs://nodemaster:9000/user/hadoop/HIGGS_Test_Scaled_500.csv').map(lambda line : [float(x) for x in line.split(',')]).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size:10500\n",
      "Test dataset size:500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset size:{}\".format(TrainingRDD.count()))\n",
    "print(\"Test dataset size:{}\".format(TestRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:green;\">Defining GP parameters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def evalHiggsBase(individual):\n",
    "    \n",
    "    # Transform the tree expression in a callable function\n",
    "    func = toolbox.compile(expr=individual)\n",
    "    \n",
    "    # Evaluate the sum of correctly identified as signal\n",
    "    result = sum(TrainingRDD.map(lambda line: bool(sigmoid(func(*line[1:]))>0.5) is bool(line[0])).collect())\n",
    "    \n",
    "    return result,\n",
    "\n",
    "# Define a new if-then-else function\n",
    "def if_then_else(input, output1, output2):\n",
    "    if input:\n",
    "        return output1\n",
    "    else:\n",
    "        return output2\n",
    "\n",
    "# Define a protected division function\n",
    "def protectedDiv(left, right):\n",
    "    try: \n",
    "        return left / right\n",
    "    except ZeroDivisionError:\n",
    "        return 1\n",
    "\n",
    "# defined a new primitive set for strongly typed GP (28 float input attributes from Higgs dataset)\n",
    "pset = gp.PrimitiveSetTyped(\"MAIN\", itertools.repeat(float, 28), bool, \"IN\")\n",
    "\n",
    "# Functions set\n",
    "\n",
    "# boolean operators\n",
    "pset.addPrimitive(operator.and_, [bool, bool], bool)\n",
    "pset.addPrimitive(operator.or_, [bool, bool], bool)\n",
    "pset.addPrimitive(operator.not_, [bool], bool)\n",
    "\n",
    "# floating point operators\n",
    "pset.addPrimitive(operator.add, [float, float], float)\n",
    "pset.addPrimitive(operator.sub, [float, float], float)\n",
    "pset.addPrimitive(operator.mul, [float, float], float)\n",
    "pset.addPrimitive(protectedDiv, [float, float], float)\n",
    "\n",
    "# logic operators\n",
    "pset.addPrimitive(operator.lt, [float, float], bool)\n",
    "pset.addPrimitive(operator.eq, [float, float], bool)\n",
    "pset.addPrimitive(if_then_else, [bool, float, float], float)\n",
    "\n",
    "# constants float in [0,1] and boolean constants False, True\n",
    "pset.addEphemeralConstant(\"rand1\", lambda: random.random(), float)\n",
    "pset.addTerminal(False, bool)\n",
    "pset.addTerminal(True, bool)\n",
    "\n",
    "#Set the best fitness as the max fitness\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)\n",
    "\n",
    "#other GP parameters : selection, mutation, ...\n",
    "toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3)\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"compile\", gp.compile, pset=pset)\n",
    "toolbox.register(\"evaluate\", evalHiggsBase)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=4)\n",
    "toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "toolbox.register(\"expr_mut\", gp.genHalfAndHalf, min_=1, max_=3)\n",
    "toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
    "toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n",
    "toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:cyan;\">Distribution over Spark</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating a population on a single exemplar\n",
    "def evalPop(individuals, line):\n",
    "    return [bool(sigmoid(i(*[float(v) for v in line[1:]]))>0.5) is bool(float(line[0])) for i in individuals]\n",
    "\n",
    "#redefinig the GP loop from DEAP to use Spark\n",
    "def eaSimple(population, toolbox, cxpb, mutpb, ngen, stats=None, halloffame=None, verbose=__debug__):\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    #higgs_samp = TrainingRDD.sample(False,sampling_rate).cache()\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    #fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    ii = [toolbox.compile(f) for f in invalid_ind]\n",
    "    \n",
    "    #results = higgs_samp.map(lambda line: evalPop(ii, line))\n",
    "    results = TrainingRDD.map(lambda line: evalPop(ii, line))\n",
    "    fitnesses = results.reduce(lambda v1,v2:list(map(operator.add,v1,v2)))\n",
    "    fitnesses = [tuple([vf]) for vf in fitnesses]\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print logbook.stream\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        # Generate new sample\n",
    "        #higgs_samp = TrainingRDD.sample(False,sampling_rate).cache()\n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the current generation individuals\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        ii = [toolbox.compile(f) for f in invalid_ind]\n",
    "        \n",
    "        results = TrainingRDD.map(lambda line: evalPop(ii, line))\n",
    "        fitnesses = results.reduce(lambda v1,v2:list(map(operator.add,v1,v2)))\n",
    "        fitnesses = [tuple([vf]) for vf in fitnesses]\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print logbook.stream\n",
    "\n",
    "    return population, logbook\n",
    "\n",
    "# Redefinition\n",
    "algorithms.eaSimple=eaSimple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(func,dataset):\n",
    "    confusion_matrix = [[0.0,0.0],[0.0,0.0]]\n",
    "    predictions=dataset.map(lambda line:[bool(sigmoid(func(*line[1:]))>0.5),bool(line[0])]).collect()\n",
    "    for line in predictions:\n",
    "        predicted = line[0]\n",
    "        real = line[1]\n",
    "        if(predicted == real and real):\n",
    "            confusion_matrix[0][0]+=1\n",
    "        elif(predicted == real and not(real)):\n",
    "            confusion_matrix[1][1]+=1\n",
    "        elif(predicted != real and real):\n",
    "            confusion_matrix[1][0]+=1\n",
    "        elif(predicted != real and not(real)):\n",
    "            confusion_matrix[0][1]+=1\n",
    "    return confusion_matrix   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:red\">Main program</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #logging to a file\n",
    "    log=open('higgs.log', 'a')\n",
    "    print(\"logs are record in file higgs.log\")\n",
    "    \n",
    "    try:\n",
    "        random.seed()\n",
    "        hof = tools.HallOfFame(1)\n",
    "\n",
    "        # Statistics\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", numpy.mean)\n",
    "        stats.register(\"std\", numpy.std)\n",
    "        stats.register(\"min\", numpy.min)\n",
    "        stats.register(\"max\", numpy.max)\n",
    "\n",
    "        #setting GP population size, mutation and crossover probabilities\n",
    "        popSize=32\n",
    "        crossover_prob = 0.9\n",
    "        mutation_prob = 0.4\n",
    "        nbGen = 5\n",
    "        pop = toolbox.population(n=popSize)\n",
    "\n",
    "        log.write('\\nRun Started\\n')\n",
    "        startTime = time.time()\n",
    "        p,lbook = algorithms.eaSimple(pop, toolbox, crossover_prob, mutation_prob, nbGen, stats, halloffame=hof, verbose=True)\n",
    "        endTime = time.time()\n",
    "        log.write(lbook.stream)\n",
    "        log.write(\"\\nLearning time : {} seconds\".format(endTime-startTime))\n",
    "\n",
    "        # Best individual\n",
    "        expr = hof[0]\n",
    "\n",
    "        # Best individual against full training set\n",
    "        func = toolbox.compile(expr)\n",
    "        confusion_matrix = confusionMatrix(func,TrainingRDD)\n",
    "        resultTraining = confusion_matrix[0][0]+confusion_matrix[1][1]\n",
    "        log.write(\"\\n\"+str(expr))\n",
    "        log.write(\"\\nfitness of best individual against total training set :{}/{}={}\".format(resultTraining, numpy.sum(confusion_matrix), float(resultTraining) / numpy.sum(confusion_matrix)))\n",
    "        cm = '\\n'.join('\\t'.join('%0.3f' %x for x in y) for y in confusion_matrix)\n",
    "        log.write(\"\\n\"+cm)\n",
    "        TP = confusion_matrix[0][0]\n",
    "        TN = confusion_matrix[1][1]\n",
    "        FP = confusion_matrix[0][1]\n",
    "        FN = confusion_matrix[1][0]\n",
    "        log.write(\"\\naccuracy ={}, TPR={}, FPR={}\".format((TP+TN)/(TP+TN+FP+FN),TP/(TP+FN),FP/(FP+TN)))\n",
    "\n",
    "        # Best individual against test dataset\n",
    "        confusion_matrix = confusionMatrix(func,TestRDD)\n",
    "        resultTest = confusion_matrix[0][0]+confusion_matrix[1][1]\n",
    "\n",
    "        log.write(\"\\nfitness of best individual against test set :{}/{}={}\".format(resultTest, numpy.sum(confusion_matrix), float(resultTest) / numpy.sum(confusion_matrix)))\n",
    "        cm = '\\n'.join('\\t'.join('%0.3f' %x for x in y) for y in confusion_matrix)\n",
    "        log.write(\"\\n\"+cm)\n",
    "        TP = confusion_matrix[0][0]\n",
    "        TN = confusion_matrix[1][1]\n",
    "        FP = confusion_matrix[0][1]\n",
    "        FN = confusion_matrix[1][0]\n",
    "        log.write(\"\\naccuracy ={}, TPR={}, FPR={}\".format((TP+TN)/(TP+TN+FP+FN),TP/(TP+FN),FP/(FP+TN)))\n",
    "        log.close()\n",
    "        return pop, stats, hof\n",
    "    except Exception as e:\n",
    "        log.write(\"Exception !: {}\".format(e))\n",
    "        log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs are record in file higgs.log\n",
      "gen\tnevals\tavg    \tstd    \tmin \tmax \n",
      "0  \t32    \t5114.97\t240.121\t4948\t5572\n",
      "1  \t31    \t5317.69\t252.733\t4955\t5661\n",
      "2  \t31    \t5328.59\t244.336\t4955\t5583\n",
      "3  \t32    \t5383.38\t254.175\t4949\t5583\n",
      "4  \t30    \t5451.75\t216.01 \t4955\t5583\n",
      "5  \t27    \t5437.94\t230.904\t4954\t5641\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}